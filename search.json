[{"title":"新手小白初试Kaggle竞赛","url":"/2024/10/22/新手小白初试Kaggle竞赛/","content":"\nhttps://www.kaggle.com/competitions/titanic\n\n\n\n>## 挑战\n>\n>泰坦尼克号沉没是历史上最臭名昭著的海难之一。\n>\n>1912 年 4 月 15 日，在处女航中，被广泛认为“永不沉没”的皇家邮轮泰坦尼克号与冰山相撞后沉没。不幸的是，救生艇数量不足以容纳船上所有人，导致 2224 名乘客和船员中有 1502 人丧生。\n>\n>尽管生存有一定的运气因素，但似乎有些人比其他人群更有可能生存下来。\n>\n>在这个挑战中，我们要求您使用乘客数据（即姓名、年龄、性别、社会经济阶层等）建立一个预测模型，回答这个问题：“什么样的人更有可能生存？”\n\n\n\n# 1. 明确数据集中的变量\n\n![image-20240925152446397](https://raw.githubusercontent.com/betty11618/images/img/202410222025557.png)\n\n\n\n# 2. 具体实现\n\n## 导包\n\n```python\nimport warnings\nwarnings.filterwarnings('ignore')  # 忽略警告信息\n\n# 数据处理清洗包\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# 可视化包\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# %matplotlib.inline\n%matplotlib inline\n\n# 机器学习算法相关包\nfrom sklearn.linear_model import LogisticRegression,Perceptron,SGDClassifier\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n```\n\n\n\n`%matplotlib.inline`   **魔法函数 magic functions**，模拟命令行，在Ipython编译器里直接使用，内嵌绘图，可忽略plt.show()\n\n官方给出的定义是：IPython有一组预先定义好的所谓的魔法函数（Magic Functions），你可以通过命令行的语法形式来访问它们。可见“%matplotlib inline”就是模仿命令行来访问magic函数的在IPython中独有的形式。\n\n> P.S. pycharm不支持\n\nmagic函数分为两种：\n\n- 面向行，前缀 `%` ，类似命令行形式\n- 面向单元 ，前缀 `%%` ，包括当前行以及以下行\n\n\n\n在这里出现一个报错，\n\n![image-20240925160219082](https://raw.githubusercontent.com/betty11618/images/img/202410222025204.png)\n\n检查一下是否安装了这个包\n\n```bash\npip show matplotlib\n```\n\n![image-20240925191617490](https://raw.githubusercontent.com/betty11618/images/img/202410222025970.png)\n\n是安装了的\n\n问了一下gpt，尝试修改为\n\n```bash\n%matplotlib inline\n```\n\n发现这就行了\n\n\n\n> 注意： `%` 后面无空格，这一行不能有注释\n\n\n\n## 载入数据集\n\n```python\ntrain_df = pd.read_csv('./train.csv')\ntest_df = pd.read_csv('./test.csv')\ncombine = [train_df, test_df]\t# 合并数据集\n```\n\n合并数据集 ---> 后面可以直接用 **for循环** 同时对训练集和测试集进行处理 \n\n\n\n## 描述性统计分析\n\n```python\nprint(train_df.columns.values)\n```\n\n获得所有特征名（列名）\n\n![image-20240925193316241](https://raw.githubusercontent.com/betty11618/images/img/202410222025971.png)\n\n\n\n预览数据\n\n```python\ntrain_df.head()\t\t# 前五行\ntrain_df.tail()\t\t# 后五行\n```\n\n前五行数据如下：\n\n![image-20240925193713811](https://raw.githubusercontent.com/betty11618/images/img/202410222025972.png)\n\n后五行数据如下：\n\n![image-20240925193821162](https://raw.githubusercontent.com/betty11618/images/img/202410222025973.png)\n\n\n\n**判断哪些特征包含缺失值（空值、NULL、NAN）**\n\n==isnull()函数 和 sum()函数==\n\n```python\nprint(train_df.isnull().sum())\t\t# 检查每一列中的缺失值数目并返回缺失值数目总和\nprint('_'*40)\t\t# 分隔训练集和测试集的统计结果\ntest_df.isnull().sum()\n```\n\n![image-20240925195422532](https://raw.githubusercontent.com/betty11618/images/img/202410222025974.png)\n\n- 在训练集中，缺失值数目 `Cabin > Age > Embarked`\n- 在测试集中，缺失值数目 `Cabin > Age > Fare`\n\n综上可知，cabin的缺失值很多，而且是字符型和数值型的混合数据，难以处理，后续选择特征时可能丢弃\n\n\n\n**预览特征的数据类型**\n\n==info()函数==\n\n```python\ntrain_df.info()\t\t# 预览每个变量的基本信息\nprint('_'*40)\ntest_df.info()\n```\n\n![image-20240925195830416](https://raw.githubusercontent.com/betty11618/images/img/202410222025975.png)\n\n|        | 整数集或浮点型 | 字符串型 |\n| :----: | :------------: | :------: |\n| 训练集 |      7个       |   5个    |\n| 测试集 |      6个       |   5个    |\n\n\n\n**样本中数据特征的分布**\n\n==discribe()函数==\n\n```python\nround(train_df.describe(percentiles=[.5,.6,.7,.75,.8,.9,.99]),2)\n```\n\n- `percentiles=[.5, .6, .7, .75, .8, .9, .99]` 是在调用 `describe()` 方法时指定的分位数，表示查看的数据分布的不同百分位数，包括中位数（50%）、60%、70%、75%、80%、90% 和99%。\n\n- `round(需要四舍五入的数字或表达式,指定小数点后保留的位数)`\n\n![image-20240925201252123](https://raw.githubusercontent.com/betty11618/images/img/202410222025976.png)\n\n分析得到的表格可以得出：\n\n![image-20240925201343106](https://raw.githubusercontent.com/betty11618/images/img/202410222025977.png)\n\n就是得出每一个特征的分布情况\n\n\n\n**查看分类特征的数据分布**\n\n```python\ntrain_df.describe(include=['O'])\t# 其中O是object（字符串型）的缩写\n```\n\n- 用于获取 `train_df` 中所有对象类型（通常是字符串类型）列的描述性统计信息。\n\n\n- 它会返回每一列的计数、唯一值的数量、最常见的值（top）以及该值的频率（freq）。\n\n\n![image-20240925201912836](https://raw.githubusercontent.com/betty11618/images/img/202410222025978.png)\n\n分析得到信息：\n\n![image-20240925202055802](https://raw.githubusercontent.com/betty11618/images/img/202410222025979.png)\n\n\n\n## 基于数据分析的假设\n\n分析每个特征与是否幸存的相关性\n\n![image-20240925202459636](https://raw.githubusercontent.com/betty11618/images/img/202410222025980.png)\n\n**先分类汇总，再可视化分析**\n\n分类汇总\n\n==groupby()函数==\n\n相当于Excel中的透视表分析\n\n```python\ntrain_df[['Pclass','Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived',ascending=False)\n```\n\n- `train_df[['Pclass','Survived']]` ：选择了这两列\n- `groupby(['Pclass'], as_index=False)` ：按 `Pclass` 列进行分组，并保持 `Pclass` 为普通列（而不是索引）\n- `mean()` ：计算每个舱位的生存率的均值\n- `sort_values(by='Survived',ascending=False)` ：按生存率降序排列\n\n![image-20240925204701703](https://raw.githubusercontent.com/betty11618/images/img/202410222025981.png)\n\n\n\n```python\ntrain_df[['Sex','Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived',ascending=False)\n```\n\n![image-20240925204831119](https://raw.githubusercontent.com/betty11618/images/img/202410222025982.png)\n\n\n\n```python\ntrain_df[['SibSp','Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived',ascending=False)\n```\n\n![image-20240925204947448](https://raw.githubusercontent.com/betty11618/images/img/202410222025983.png)\n\n\n\n```python\ntrain_df[['Parch','Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived',ascending=False)\n```\n\n![image-20240925205103176](https://raw.githubusercontent.com/betty11618/images/img/202410222025984.png)\n\n分析：\n\n![image-20240925205353738](https://raw.githubusercontent.com/betty11618/images/img/202410222025985.png)\n\n注意：`SibSp` 和 `Parch` 中有的值与是否幸存是零相关性，所以我们最好的做法是将这两个特征结合成一个新的特征，使新的特征与是否幸存有显著性的相关性\n\n\n\n## 可视化数据分析\n\n> 分析Age与是否幸存相关性\n\n```python\ng = sns.FacetGrid(train_df,col='Survived')\ng.map(plt.hist,'Age',bins=20)\n```\n\n- `sns.FacetGrid(train_df, col='Survived')`：根据 `Survived` 列的值，创建多个子图\n- `g.map(plt.hist, 'Age', bins=20)`：在每个子图中绘制 `Age` 的直方图，并将直方图的箱数设置为 20\n\n![image-20240925205916639](https://raw.githubusercontent.com/betty11618/images/img/202410222025986.png)\n\n分析：\n\n![image-20240925205828277](https://raw.githubusercontent.com/betty11618/images/img/202410222025987.png)\n\n\n\n> 分析Pclass与是否幸存相关性\n\n```python\ngrid = sns.FacetGrid(train_df,col='Pclass',hue='Survived')\ngrid.map(plt.hist,'Age',alpha=0.5,bins=20)\ngrid.add_legend();\n```\n\n- `grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')` 根据 `Pclass` 列的值创建多个子图，同时根据 `Survived` 列的值为数据点上色\n- `grid.map(plt.hist, 'Age', alpha=0.5, bins=20)` 在每个子图中绘制 `Age` 的直方图，`alpha=0.5` 设置了透明度，使得重叠部分可以更清晰地显示。`bins=20` 表示直方图的箱数为 20\n- `grid.add_legend();` 添加图例，便于区分不同的生存状态\n\n![image-20240925210915984](https://raw.githubusercontent.com/betty11618/images/img/202410222025988.png)\n\n分析年龄、票价等级、是否幸存三者的关系：\n\n![image-20240925211221241](https://raw.githubusercontent.com/betty11618/images/img/202410222025989.png)\n\n\n\n> 分析Embarked与是否幸存相关性\n\n```python\ngrid = sns.FacetGrid(train_df,col='Embarked')\ngrid.map(sns.pointplot,'Pclass','Survived','Sex',palette='deep')\ngrid.add_legend();\n```\n\n- `grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')` 在每个子图中绘制点图，`x` 轴是 `Pclass`，`y` 轴是 `Survived`，点的颜色根据 `Sex` 列分组。`palette='deep'` 设置颜色调色板\n\n\n\n![image-20240925211731782](https://raw.githubusercontent.com/betty11618/images/img/202410222025990.png)\n\n分析不同的登船港口下，舱位与幸存率的关系，并且按性别进行区分：\n\n![image-20240925211804997](https://raw.githubusercontent.com/betty11618/images/img/202410222025991.png)\n\n\n\n> 分析Fare与是否幸存相关性\n\n```python\ngrid = sns.FacetGrid(train_df,col='Embarked',hue='Survived',palette={0:'b',1:'r'})\ngrid.map(sns.barplot,'Sex','Fare',alpha=.5,ci=None)\ngrid.add_legend()\n```\n\n- `palette={0:'b',1:'r'}` 指定生存状态的颜色（0 为蓝色，1 为红色）\n- `grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)` 绘制条形图，`x` 轴是 `Sex`，`y` 轴是 `Fare`。`alpha=.5` 设置透明度，`ci=None` 关闭置信区间的绘制\n\n![image-20240925212652875](https://raw.githubusercontent.com/betty11618/images/img/202410222025992.png)\n\n分析：\n\n![image-20240925212302590](https://raw.githubusercontent.com/betty11618/images/img/202410222025993.png)\n\n\n\n## 整理、清洗数据\n\n**删除无用特征 Ticket 和 Cabin**\n\n```python\nprint(\"Before\",train_df.shape,test_df.shape,combine[0].shape,combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket','Cabin'],axis=1)\ntest_df = test_df.drop(['Ticket','Cabin'],axis=1)\ncombine = [train_df,test_df]\n\n\"After\",train_df.shape,test_df.shape,combine[0].shape,combine[1].shape\n```\n\n- `xxx.shape` 显示xxx数据集的形状（行数，列数）\n- `xxx.drop` 删除列\n\n![image-20240925214012562](https://raw.githubusercontent.com/betty11618/images/img/202410222025994.png)\n\n\n\n**从现有特征中提取新特征**\n\n```python\ntrain_df['Name'].head(10)\n```\n\n![image-20240926153351587](https://raw.githubusercontent.com/betty11618/images/img/202410222025995.png)\n\n从以上的结果我们可以分析出 `Name` 特征中其实是含有一部分 `Title` 特征的，那么我们是否可以提取 `Title` 特征来判断其与是否幸存的相关性呢。\n\n在这里我们可以使用 **正则表达式** 来提取 `Title` 特征，\n\n```python\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\\.',expand=False)\n    \npd.crosstab(train_df['Title'],train_df['Sex']).sort_values(by=\"female\",ascending=False)\n```\n\n- `for dataset in combine:\n      dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\\.', expand=False)` 对 `combine` 列表中的每个数据框进行操作，从 `Name` 列中提取以点号（`.`）为结束的称谓，并将其存储在新列 `Title` 中\n- `pd.crosstab(train_df['Title'], train_df['Sex']).sort_values(by=\"female\", ascending=False)` 生成一个交叉表，显示不同称谓和性别（`Sex`）的计数，并按女性（`female`）的计数降序排序\n\n![image-20240926154727914](https://raw.githubusercontent.com/betty11618/images/img/202410222025996.png)\n\n\n\n分析 `Title` 、`Age` 和是否幸存三者之间的关系\n\n```python\ngrid = sns.FacetGrid(train_df,col='Title',hue='Survived',col_wrap=3,size=2.5,aspect=1.6)\ngrid.map(plt.hist,'Age',alpha=0.5,bins=20)\ngrid.add_legend()\n```\n\n- `col='Title'`：按称谓分列\n\n  `hue='Survived'`：根据生存状态上色\n\n  `col_wrap=3`：每行最多显示 3 列\n\n  `height=2.5` 和 `aspect=1.6`：设置每个子图的高度和宽高比。\n\n![image-20240926160044855](https://raw.githubusercontent.com/betty11618/images/img/202410222025997.png)\n\n发现有些 `Title` 很少，我们可以合并这些稀有的 `Title` \n\n```python\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'],'Rare')\n    dataset['Title'] = dataset['Title'].replace(['Mlle','Ms'],'Miss')\n    dataset['Title'] = dataset['Title'].replace(['Mme','Mrs'])\n    \ntrain_df[['Title','Survived']].groupby(['Title'],as_index=False).mean() \n```\n\n-  `train_df[['Title','Survived']].groupby(['Title'],as_index=False).mean() ` 按 `Title` 分组，并计算每个称谓的平均生存率\n\n![image-20240926161705224](https://raw.githubusercontent.com/betty11618/images/img/202410222025998.png)\n\n\n\n**把分类标题转化为系数**\n\n```python\ntitle_mapping = {\"Mr\":1,\"Miss\":2,\"Mrs\":3,\"Master\":4,\"Rare\":5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \ntrain_df.head()\n```\n\n- `title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}`：创建了一个字典，将每个称谓映射到一个数值\n- `dataset['Title'].map(title_mapping)`：将 `Title` 列中的称谓替换为对应的数值\n- `dataset['Title'].fillna(0)`：将缺失值填充为 0\n\n\n\n![image-20240926165429128](https://raw.githubusercontent.com/betty11618/images/img/202410222025999.png)\n\n\n\n**从训练集和测试集中删除Name特征以及训练集中的PassengerId特征**\n\n```python\ntrain_df = train_df.drop(['Name','PassengerId'],axis=1)\ntest_df = test_df.drop(['Name'],axis=1)\t# 删除列\ncombine = [train_df,test_df]\ntrain_df.shape,test_df.shape\n```\n\n\n\n![image-20240926170328977](https://raw.githubusercontent.com/betty11618/images/img/202410222025000.png)\n\n\n\n**转换性别特征Sex**\n\n```python\nfor dataset in combine:\n        dataset['Sex'] = dataset['Sex'].map( {'female':1,'male':0} ).astype(int)\n        \ntrain_df.head()\n```\n\n\n\n![image-20240926171156359](https://raw.githubusercontent.com/betty11618/images/img/202410222025001.png)\n\n\n\n**填补 Age 的缺失值**\n\n填补缺失值的常见做法：运用中位数或均值直接填补\n\n**绘制Age、Pclass和Sex的复合直方图**\n\n```python\ngrid = sns.FacetGrid(train_df,col='Pclass',hue='Sex')\ngrid.map(plt.hist,'Age',alpha=0.5,bins=20)\ngrid.add_legend()\n```\n\n\n\n![image-20240926174134905](https://raw.githubusercontent.com/betty11618/images/img/202410222025002.png)\n\n注意到 `Age`、 `Sex`、 `Pclass` 之间有关系\n\n![image-20240926171449868](https://raw.githubusercontent.com/betty11618/images/img/202410222025003.png)\n\n\n\n法一：\n\n\n\n```python\n# 创建两行三列空数组\nguess_ages = np.zeros((2,3))\nguess_ages\n\n# 遍历Sex(0/1)和Pclass(1/2/3)来计算六种组合的Age猜测值\nfor dataset in combine:\n    # 第一个for循环计算每一个分组的Age预测值\n    for i in range(0,2):\n        for j in range(0,3):\n            # guess_df 时每一个分组下非缺失值的 Age 序列\n            guess_df = dataset[(dataset['Sex'] == i)&(dataset['Pclass'] == j+1)]['Age'].dropna()\n            \n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n            \n            age_guess = guess_df.median()  # 求每个序列的中位数\n            \n            #将随机年龄浮点数转换为最接近的 0.5 年龄（四舍五入）\n            guess_ages[i,j] = int(age_guess/0.5 + 0.5) * 0.5\n            \n    # 第二个for循环对空值进行赋值\n    for i in range(0,2):\n        for j in range(0,3):\n            dataset.loc[(dataset.Age.isnull())&(dataset.Sex == i)&(dataset.Pclass == j+1),'Age'] = guess_ages[i,j]\n    \n    dataset['Age'] = dataset['Age'].astype(int)  # 将浮点型预测值转化为整形，便于观察\ntrain_df.head()\n```\n\n\n\n![image-20240926180049716](https://raw.githubusercontent.com/betty11618/images/img/202410222025004.png)\n\n\n\n对其进行分组（分箱操作） ---> ==降低模型过拟合的风险==\n\n```python\n# 创建年龄段，并确定其与是否幸存的相关性\n# 一般在建立分类模型时，需要对连续变量离散化，特征离散化后，模型会更稳定，降低了模型过拟合的风险\ntrain_df['AgeBand'] = pd.cut(train_df['Age'],5)  # 将年龄分割为5段，等距分组\ntrain_df[['AgeBand','Survived']].groupby(['AgeBand'],as_index=False).mean().sort_values(by='AgeBand',ascending=True)\n```\n\npandas 中的 `cut()`函数 ---> ==等距分箱==（在本例中将0-80的年龄等距分成五段，再与是否幸存进行分类汇总）\n\n![image-20240926234305774](https://raw.githubusercontent.com/betty11618/images/img/202410222025005.png)\n\n\n\n**将年龄段序数化**\n\n```python\n# 将这些年龄区间替换为序数\nfor dataset in combine:\n    dataset.loc[dataset['Age'] <= 16,'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16)&(dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32)&(dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48)&(dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[dataset['Age'] > 64,'Age'] = 4\ntrain_df.head()\n```\n\n\n\n![image-20240926235549204](https://raw.githubusercontent.com/betty11618/images/img/202410222025006.png)\n\n\n\n**删除训练集中生成的 AgeBand 特征**\n\n```python\ntrain_df = train_df.drop(['AgeBand'],axis=1)  # 删除训练集中的AgeBand特征\ncombine = [train_df,test_df]\ntrain_df.head()\ntest_df\n```\n\n\n\n![image-20240927000010400](https://raw.githubusercontent.com/betty11618/images/img/202410222025007.png)\n\n\n\n**结合 SibSp 和 Parch 特征创建一个新特征 FamilySize （包括兄弟姐妹、配偶、父母、孩子和自己所有家人的数量）**\n\n```python\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\t\t# 加上的1是自己\n    \n# 计算平均生存率\ntrain_df[['FamilySize','Survived']].groupby(['FamilySize'],as_index=False).mean().sort_values(by='Survived',ascending=False)\n```\n\n\n\n![image-20240928154046573](https://raw.githubusercontent.com/betty11618/images/img/202410222025008.png)\n\n分析以上数据，可以观察到\n\n-  `FamilySize` 和 是否幸存之间有一定的相关性\n-  `8  11` 这行数据与是否幸存是零相关性的 ---> ==创建一个新特征 `IsAlone`==\n\n\n\n**IsAlone 取值为0：不是独自一人，取值为1：独自一人**\n\n再次进行分类汇总\n\n```python\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1,'IsAlone'] =1\n    \ntrain_df[['IsAlone','Survived']].groupby(['IsAlone'],as_index=False).mean()\n```\n\n\n\n![image-20240928155227415](https://raw.githubusercontent.com/betty11618/images/img/202410222025009.png)\n\n分析数据可知，不是独自一人幸存率高，独自一人幸存率低\n\n这样新特征与是否幸存相关率高了，这样我们就可以删除 `Parch` `SibSp` `FamilySize` 这三个特征了，转化成了 `IsAlone` 这一个相关性高的特征\n\n\n\n**删除 SibSp Parch FamilySize 这三个特征**\n\n```python\ntrain_df = train_df.drop(['Parch','SibSp','FamilySize'],axis=1)\ntest_df = test_df.drop(['Parch','SibSp','FamilySize'],axis=1)\ncombine = [train_df,test_df]\n\ntrain_df.head()\n```\n\n\n\n![image-20240928160144881](https://raw.githubusercontent.com/betty11618/images/img/202410222025010.png)\n\n\n\n**创建人工特征 Age 和 Pclass 的交互项，Age*Pclass ，结合 Age 和 Pclass 变量**\n\n**构建交互项**常见于计量分析，在回归模型中可以更好更有效的分析这个解释变量与被解释变量之间影响的一个作用机制\n\n```python\nfor dataset in combine:\n    dataset['Age*Pclass'] = dataset.Age * dataset.Pclass\n    \ntrain_df.loc[:,['Age*Pclass','Age','Pclass']].head(10)\ntrain_df[['Age*Pclass','Survived']].groupby(['Age*Pclass'],as_index=False).mean()\n```\n\n\n\n![image-20240928160815707](https://raw.githubusercontent.com/betty11618/images/img/202410222025011.png)\n\n\n\n**填补分类特征 Embarked **\n\n只在训练集中有两个缺失值 ，用 **众数** 填补\n\n```python\nfreq_port = train_df.Embarked.dropna().mode()[0]\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean().sort_values(by='Survived',ascending=False)\n```\n\n\n\n![image-20240928161551328](https://raw.githubusercontent.com/betty11618/images/img/202410222025012.png)\n\n\n\n**将其序数化**\n\n```python\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map({'S':0,'C':1,'Q':2}).astype(int)\n    \ntrain_df.head()\n```\n\n\n\n![image-20240928161825903](https://raw.githubusercontent.com/betty11618/images/img/202410222025013.png)\n\n\n\n**对 Fare 特征进行分箱并替换为序数**\n\n在测试集中有一个缺失值，用测试集中 Fare的 **中位数** 填补\n\n```python\ntest_df['Fare'].fillna(test_df['Fare'].dropna().median(),inplace=True)\ntest_df.head()\n```\n\n\n\n![image-20240928162221698](https://raw.githubusercontent.com/betty11618/images/img/202410222025014.png)\n\n\n\n`Fare` 是一个连续变量，所以需要以一个分箱操作\n\n```python\nplt.hist(train_df['Fare'])\n```\n\n\n\n![image-20240928162506661](https://raw.githubusercontent.com/betty11618/images/img/202410222025015.png)\n\n分析可知，`Fare` 呈现一个 **右偏分布**，主要分布在0-100之间 ---> ==采用等频分箱== ---> ==pandas中的 `qcut()`函数实现==\n\n```python\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'],4)\ntrain_df[['FareBand','Survived']].groupby(['FareBand'],as_index=False).mean().sort_values(by='FareBand',ascending=True)\n```\n\n根据相同的样本分位数分为4份\n\n![image-20240928163227925](https://raw.githubusercontent.com/betty11618/images/img/202410222025016.png)\n\n分析可知，票价越高，幸存率越高\n\n\n\n**每个分段区间序数化**\n\n```python\nfor dataset in combine:\n    dataset.loc[dataset['Fare'] <= 7.91,'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91)&(dataset['Fare'] <= 14.454),'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454)&(dataset['Fare'] <= 31),'Fare'] = 2\n    dataset.loc[dataset['Fare'] > 31,'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \ntrain_df = train_df.drop(['FareBand'],axis=1)\ncombine = [train_df,test_df]\n\ntrain_df.head(10)\n```\n\n\n\n![image-20240928164059837](https://raw.githubusercontent.com/betty11618/images/img/202410222025017.png)\n\n![image-20240928164207142](https://raw.githubusercontent.com/betty11618/images/img/202410222025018.png)\n\n\n\n综上所述，总共得到了八个与是否幸存相关的特征\n\n\n\n## 模型构建与预测\n\n==分类和回归问题==\n\n![image-20240928172717989](https://raw.githubusercontent.com/betty11618/images/img/202410222025019.png)\n\n\n\n首先划分训练集特征、对应的类标签、测试集特征\n\n然后用以上的模型去训练，并进行一个模型预测\n\n\n\n```python\nX_train = train_df.drop(\"Survived\",axis=1)\nY_train = train_df[\"Survived\"]\nX_test = test_df.drop(\"PassengerId\",axis=1).copy()\nX_train.shape,Y_train.shape,X_test.shape\n```\n\n\n\n![image-20240928173313379](https://raw.githubusercontent.com/betty11618/images/img/202410222025020.png)\n\n\n\n### **逻辑回归模型**\n\n通过 `sigmoid()`函数（逻辑回归函数）将逻辑回归问题转换为一个简单的**线性回归**问题，就可以用来测量分类因变量和一个或多个自变量之间的关系。 常见的就是用来处理二分类问题，**我们需要关注的是模型基于训练集模型生成的置信度分数**\n\n\n\n```python\n# 逻辑回归模型\nlogreg = LogisticRegression()\nlogreg.fit(X_train,Y_train)\t\t# 用训练集特征和对应的类标签拟合逻辑回归模型\ny_pred = logreg.predict(X_test)\t\t# 用测试集的特征进行预测，预测测试集的一个类标签\nacc_log = round(logreg.score(X_train,Y_train) * 100, 2)\t\t# 用score函数评价模型的好坏\nacc_log\n```\n\n![image-20241021145142030](https://raw.githubusercontent.com/betty11618/images/img/202410222025021.png)\n\n\n\n逻辑回归本质上是做一个分类，但也是一个回归模型\n\n可以使用逻辑回归中特征的一个系数，去研究各个特征对于 `Survived` 的影响程度\n\n![image-20241021150740053](https://raw.githubusercontent.com/betty11618/images/img/202410222025022.png)\n\n\n\n```python\ncoeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)\n```\n\n通过 `logreg.coef` 看每一个特征对应的系数\n\n![image-20241021150502775](https://raw.githubusercontent.com/betty11618/images/img/202410222025023.png)\n\n通过结果，可以发现\n\n![image-20241021150727875](https://raw.githubusercontent.com/betty11618/images/img/202410222025024.png)\n\n\n\n### **支持向量机**\n\n支持向量机是一类按**监督学习方式**对数据进行**二元分类**的**广义线性分类器**，他的决策边界是对学习样本求解的**最大边距超平面**。\n\n最为常见的是**通过核函数的方法进行非线性分类**。\n\n```python\n# 支持向量机模型\nsvc = SVC()\nsvc.fit(X_train,Y_train)\ny_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train,Y_train) * 100, 2)\nacc_svc\n```\n\n![image-20241021152048671](https://raw.githubusercontent.com/betty11618/images/img/202410222025025.png)\n\n该模型生成的置信分数**高于**逻辑回归模型。\n\n\n\n### **KNN**\n\nk近邻算法是一种用于分类和回归的非参数方法\n\n基本思想：在特征空间中，如果一个样本附近的k个最近样本的大多数属于某一个类别，则该样本也属于这个类别。\n\n\n\n```python\n# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn\n```\n\n\n\n![image-20241021152115685](https://raw.githubusercontent.com/betty11618/images/img/202410222025026.png)\n\nKNN算法置信分数高于逻辑回归、支持向量机\n\n\n\n### **朴素贝叶斯分类器**\n\n是一系列以假设**特征之间强独立**下运用贝叶斯定理为基础的简单概率分类器。\n\n```python\n# 朴素贝叶斯分类器\ngaussian = GaussianNB()\ngaussian.fit(X_train,Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train,Y_train)*100,2)\nacc_gaussian\n```\n\n![image-20241021192113153](https://raw.githubusercontent.com/betty11618/images/img/202410222025027.png)\n\n生成的置信分数是目前评估之中最低的\n\n\n\n### **感知机**\n\n感知机是一种用于**监督学习二元分类**的算法（可以决定由数字向量表示的输入是否属于某个特定类的函数）。他是一种**线性分类器**，即基于将一组权重与特征向量相结合的线性预测函数进行预测的分类算法\n\n数据集是线性可分的，那么感知机学习的目标是：求解一个能够将训练集正实例点和负实例点完全正确分开的一个超平面\n\n求解感知机模型 ---> 求解线性可分支持向量机的一个过程，再应用硬间隔最大化，就成为了线性可分支持向量机\n\n\n\n```python\n# 感知机\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron\n```\n\n\n\n![image-20241021193603200](https://raw.githubusercontent.com/betty11618/images/img/202410222025028.png)\n\n置信得分最低\n\n---> 数据不是线性可分的\n\n\n\n### **决策树**\n\n核心思想：在一个训练集中找到一个最优特征，然后从这个特征的候选值中找到一个最优的候选值，根据这个最优的候选值把数据分为两个子数据集，然后递归上述操作，直到满足条件为止\n\n将**特征（树枝）映射到目标值（树叶）**的分类或回归方法。目标变量可以取一组**有限值**的树模型称为分类树；在这些树结构中，**叶子代表类标签**，**分支代表导致这些类标签的特征的结合**。目标变量可以取连续值的决策树称为回归树。\n\n```python\n# 决策树\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree\n```\n\n![image-20241021200312673](https://raw.githubusercontent.com/betty11618/images/img/202410222025029.png)\n\n评估得分最高，有很好的分类效果\n\n\n\n**决策树可视化**\n\n```python\n# 决策树可视化\nfrom sklearn import tree\nimport graphviz\nimport pydotplus\nfrom IPython.display import Image\ndot_data = tree.export_graphviz(decision_tree, out_file=None,\n                               feature_names=X_train.columns,\n                               class_names=['0','1'],\n                               max_depth=3,\n                               filled=True, rounded=True,\n                               special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data)\nImage(graph.create_png())\n```\n\n- `dot_data = tree.export_graphviz(...)`：用 `tree.export_graphviz()` 函数将决策树导出为 `.dot` 格式的数据流\n  - `decision_tree` ：我们的决策树模型\n  - `out_file=None` ：不输出写到文件中\n  - `feature_names=X_train.columns` ：传入特征的名称列表， `X_train` 是训练数据集， `X_train.columns` 是特征的名称\n  - `class_names=['0','1']` ：分类标签的名称，幸存为没有幸存为0\n  - `max_depth=3` ：决策树的深度，为看清图片，设置为3\n  - `filled=True` ：节点根据分类概率进行颜色填充\n  - `rounded=True` ：节点的形状为圆角矩阵\n  - `special_characters=True` ：启用特殊字符支持\n- `graph = pydotplus.graph_from_dot_data(dot_data)` ：用 `pydotplus` 库的 `graph_from_dot_data()` 函数将 `.dot` 格式的数据转换为一个可以处理的图结构（`graph` 对象）。\n- `Image(graph.create_png())` ：用 `graph.create_png()` 将图结构转换为 PNG 格式的图像。`Image()` 函数用于在 Jupyter Notebook 中显示生成的 PNG 图像。\n\n\n\n这里会出现一个报错\n\n> InvocationException: GraphViz's executables not found\n\n不是没有安装这个包，Graphviz不是一个python tool，而是要单独安装 `GraphViz` \n\nhttps://www.graphviz.org/\n\n下载后，配置环境变量\n\n```bash\ndot -version\n```\n\n检验是否安装配置成功\n\n\n\n![image-20241022091641684](https://raw.githubusercontent.com/betty11618/images/img/202410222025030.png)\n\n- 根节点是 `Sex` 特征\n- `gini` 是指 **基尼系数** ，基尼系数越小，分类越纯，划分得越好\n- `sample` 指的是样本数\n- `value` 指的是类标签对应的样本数\n- 左边为 `True` ，右边为 `False`\n\n\n\n```python\ndot_data = tree.export_graphviz(decision_tree, out_file=None,\n                               feature_names=X_train.columns,\n                               filled=True, rounded=True,\n                               special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data)\ngraph.write_pdf(\"DTree.pdf\")\n```\n\n导出完整决策树\n\n![image-20241022103021568](https://raw.githubusercontent.com/betty11618/images/img/202410222025031.png)\n\n\n\n```python\ntrain_df[(train_df['Sex']<=0.5) & (train_df['Age*Pclass']>1.5) & (train_df['Pclass']>1.5) & (train_df['Age']<=1.5) & (train_df['Fare']<=2.5) & (train_df['Title']<=2.5) & (train_df['Embarked']>0.5) & (train_df['Embarked']<=1.5) & (train_df['IsAlone']>0.5) & (train_df['Fare']>0.5) & (train_df['Fare']<=1.5) & (train_df['Age*Pclass']<=2.5)]\n```\n\n根据决策树去检索样本，验证决策树对于样本的分类\n\n![image-20241022103111223](https://raw.githubusercontent.com/betty11618/images/img/202410222025032.png)\n\n\n\n### **随机森林**\n\n基于 `bagging` 的一种集成学习方法，用于分类、回归和其他任务。\n\n通过**自助重采样技术**，从原始训练样本集中**有放回地**重复随机抽取 n 个样本生成**新的训练样本集合**训练决策树，然后按照以上步骤生成 m 棵决策树组成**随机森林**，新数据的分类结果按**分类树投票多少形成的分数**而定。\n\n实质是对决策树算法地一种改进，将多个决策树合并在一起，每棵树的建立依赖于独立抽取的样本。\n\n\n\n```python\n# 随机森林\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest\n```\n\n\n\n![image-20241022104457252](https://raw.githubusercontent.com/betty11618/images/img/202410222025033.png)\n\n\n\n## 模型评估\n\n对所有的模型评估结果进行排名，选择最适合问题的模型\n\n```python\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machine', 'KNN', 'Logistic Regression', \n             'Random Forest', 'Naive Bayes', 'Perceptron','Decision'],\n    'Score': [acc_svc, acc_knn, acc_log,\n             acc_random_forest, acc_gaussian, acc_perceptron, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)\n```\n\n\n\n![image-20241022105553995](https://raw.githubusercontent.com/betty11618/images/img/202410222025034.png)\n\n决策树和随机森林的得分相同，但是我们选择随机森林（决策树的改进版），因为他纠正了决策树过度拟合训练集带来的缺陷\n\n\n\n## 提交预测结果\n\n```python\nsubmission = pd.DataFrame({\n    \"PassengerId\": test_df[\"PassengerId\"],\n    \"Survived\": Y_pred\n})\nsubmission.to_csv('./submission.csv', index=False)\n```\n\n\n\n\n\n> 需要关注的问题\n\n- 为什么要从名字这种文本数据中提取特征？\n\n**挖掘潜在信息**\n\n**增加特征维度**\n\n\n\n- 填补年龄特征时，为什么不直接用均值或者中位数去填补，而用相关特征分为6组分别填补？\n\n**考虑特征相关性**\n\n**提高数据准确性**\n\n**减少模型偏差**\n\n\n\n- `SipSp` 和 `Parch` 特征是数据集中比较好的数据了，为什么还需要把他们合并，构造一个新的特征呢？\n\n**提升特征相关性**\n\n**简化模型和避免多重共线性**\n\n\n\n- 年龄特征分段为什么采用的是等距分箱？而票价特征就采用等频分箱呢？\n\n年龄特征采用等距分箱的原因：\n\n**自然年龄区间的考虑**\n\n**数据分布和模型稳定性**\n\n票价特征采用等频分箱的原因：\n\n**票价数据的偏态分布**\n\n**突出票价差异对幸存率的影响**\n\n\n\n总结：\n\n![img](https://raw.githubusercontent.com/betty11618/images/img/202410222025035.png)\n\n纯种小白的第一次kaggle竞赛尝试，纯纯跟着视频敲的，最后提交的得分不高，课堂上讲的纯理论也不太跟得上，还是下来以赛促学吧，再了解了解理论，好歹也算是在今天敲到机器学习的门槛了，接下来的日子多多加油吧！！！\n\n2024年10月22日\n\n\n\n参考资料：\n\n【【建议收藏】机器学习实战 泰坦尼克号幸存者预测 | Kaggle竞赛实例系列】https://www.bilibili.com/video/BV11F411u7Ck?vd_source=77e1fe7a7419fa7dbd62746624c75975\n\nhttps://blog.csdn.net/liangzuojiayi/article/details/78183783"},{"title":"机器学习入门","url":"/2024/09/23/机器学习入门/","content":"\n# 入门\n\n[一文看懂机器学习「3种学习方法+7个实操步骤+15种常见算法」 (easyai.tech)](https://easyai.tech/ai-definition/machine-learning/)\n\n> 机器学习不是某种算法，而是很多算法的统称。\n\n深度学习就是其中的一种算法\n\n\n\n![image-20240919211954341](https://raw.githubusercontent.com/betty11618/images/img/202409231558962.png)\n\n\n\n# 基本思路\n\n将问题转化为数学问题，然后解答数学题\n\n![image-20240919212102600](https://raw.githubusercontent.com/betty11618/images/img/202409231558963.png)\n\n最为困难的部分是抽象\n\n\n\n# 实现原理\n\n>训练集\n>\n>特征\n>\n>建模\n>\n>模型\n\n![image-20240919212404716](https://raw.githubusercontent.com/betty11618/images/img/202409231558964.png)\n\n机器学习：**通过训练集，不断识别特征，不断建模，最后形成有效的模型**\n\n\n\n# 分类\n\n根据**训练方法**分：\n\n- 监督学习\n- 非监督学习\n- 强化学习\n\n\n\n## 监督学习\n\n给算法一个**数据集**，并给出**正确答案**\n\n机器**直接学习正确答案**的计算方法\n\n![image-20240919212706094](https://raw.githubusercontent.com/betty11618/images/img/202409231558965.png)\n\n\n\n### 优劣势\n\n- 学习效果好\n- 成本高\n\n\n\n### 特点\n\n需要有明确的目标，清楚想要的结果\n\n\n\n### 流程\n\n1. 选择适合目标任务的数学模型\n2. 将一部分已知的“问题和答案”（训练集）给机器去学习\n3. 机器总结出自己的方法论\n4. 将“新的问题”（测试集）给机器解答\n\n![image-20240919213143745](https://raw.githubusercontent.com/betty11618/images/img/202409231558966.png)\n\n\n\n### 任务\n\n- 回归：预测**连续的**、具体的数值\n- 分类：对各种事物分门别类，用于**离散型预测**\n\n![image-20240919213406599](https://raw.githubusercontent.com/betty11618/images/img/202409231608039.png)\n\n\n\n举栗子\n\n#### 回归\n\n1. 构建问题，选择模型\n   - 找影响因素来构建模型![image-20240919213713294](https://raw.githubusercontent.com/betty11618/images/img/202409231608394.png)\n2. 收集已知数据\n   - 分数据![image-20240919213824310](https://raw.githubusercontent.com/betty11618/images/img/202409231608570.png)\n3. 训练出理想模型，验证公式\n   - 将这五种数据套入公式，计算出信用分\n   - 用计算出来的信用分跟实际的信用分（预先准备好的）进行比较\n   - 评估公式的准确度，如果问题很大再进行调整优化\n4. 对新用户进行预测\n\n\n\n#### 分类\n\n1. 构建问题，选择模型\n2. 收集已知数据\n3. 训练出理想模型\n4. 对新用户进行预测\n\n\n\n### 主流的监督学习算法\n\n![image-20240921150747467](https://raw.githubusercontent.com/betty11618/images/img/202409231558970.png)\n\n\n\n## 非监督学习\n\n给定的数据集没有 **正确答案** ，所有的数据都是一样的。\n\n\n\n### 任务\n\n从给定的数据集中，挖掘出潜在的结构\n\n\n\n![image-20240921151406473](https://raw.githubusercontent.com/betty11618/images/img/202409231558971.png)\n\n将**不打标签**的照片给机器\n\n![image-20240921151512658](https://raw.githubusercontent.com/betty11618/images/img/202409231558972.png)\n\n机器通过学习会将照片分为两类，**但是机器不知道哪个是猫哪个是狗** ，只是分为两类\n\n\n\n### 对比监督学习\n\n|      监督学习      |                   无监督学习                   |\n| :----------------: | :--------------------------------------------: |\n| 目的明确的训练方法 | 没有明确目的的训练方法，无法提前知道结果是什么 |\n|    给数据打标签    |               不需要给数据打标签               |\n|    可以衡量结果    |              几乎无法量化效果如何              |\n\n![image-20240921152240402](https://raw.githubusercontent.com/betty11618/images/img/202409231558973.png)\n\n无监督学习是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。\n\n\n\n### 特点\n\n- 没有明确的目的\n- 不需要给数据打标签\n- 无法量化结果\n\n\n\n### 使用场景\n\n- 案例一：发现异常，分类速排出正常用户，更有针对性的对异常的洗钱行为进行深入分析\n- 案例二：用户细分，通过用户行为对用户进行分类\n- 案例三：推荐系统，淘宝根据浏览行为进行分类，发现购买行为相似的用户，推荐这类用户喜欢的商品\n\n\n\n### 常见的两类无监督学习算法\n\n- 聚类：一种自动分类的方法，**不清楚聚类后的几个分类每个代表什么意思**\n- 降维：看上去很像压缩，这是为了在尽可能保存相关的结构的同时降低数据的复杂度\n\n\n\n#### 聚类算法：k均值聚类\n\n![image-20240921154448347](https://raw.githubusercontent.com/betty11618/images/img/202409231558974.png)\n\n\n\n#### 聚类算法：层次聚类\n\n适用于不知道应该分几类的情况\n\n层次聚类会构建一个多层嵌套的分类，类似一个树状结构\n\n![image-20240921154704883](https://raw.githubusercontent.com/betty11618/images/img/202409231558975.png)\n\n\n\n##### 步骤\n\n![image-20240921154751793](https://raw.githubusercontent.com/betty11618/images/img/202409231558976.png)\n\n\n\n#### 降维算法：主成分分析 - PCA\n\n主成分分析：把多指标转化为少数几个综合指标\n\n主成分分析经常用 **减少数据集的维数，同时保持数据集的对方差** 贡献最大的特征 。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能保留住数据的最重要方面。\n\n##### 变换的步骤\n\n![image-20240921155632063](https://raw.githubusercontent.com/betty11618/images/img/202409231558977.png)\n\n\n\n#### 降维算法：奇异值分解 - SVD\n\n线性代数中的一种重要的矩阵分解，奇异值分解则是特征分解 在任意矩阵上的推广\n\n\n\n#### 生成模型和GAN\n\n![image-20240921160244876](https://raw.githubusercontent.com/betty11618/images/img/202409231558978.png)\n\n\n\n## 强化学习                                        \n\n一类算法的统称\n\n\n\n### 思路\n\n类似于绩效奖励\n\n\n\n>代理\n>\n>目标\n>\n>环境\n>\n>行动\n>\n>奖励\n\n![image-20240923151556745](https://raw.githubusercontent.com/betty11618/images/img/202409231558979.png)\n\n\n\n### 区别\n\n**不需要大量的“数据喂养”**，而是自己不停的尝试\n\n\n\n### 主流算法\n\n两大分类，主要差异为 **智能体是否能完整了解或学习到所在环境的模型**\n\n| 分类 | 免模型学习                                           |                   有模型学习                   |\n| ---- | ---------------------------------------------------- | :--------------------------------------------: |\n| 优点 | 对环境有提前的认知，可以提前考虑规划                 | 更加容易实现，容易在真实场景下调整到很好的状态 |\n| 缺点 | 如果模型跟真实世界不一致，在实际使用场景下表现得不好 |               效率不如免模型学习               |\n\n**免模型学习方法更受欢迎，得到更加广泛的开发和测试**\n\n\n\n![image-20240923152453236](https://raw.githubusercontent.com/betty11618/images/img/202409231558980.png)\n\n\n\n#### 免模型学习：策略优化\n\n![image-20240923152631723](https://raw.githubusercontent.com/betty11618/images/img/202409231558981.png)\n\n\n\n#### 免模型学习：Q-Learning\n\n![image-20240923152716326](https://raw.githubusercontent.com/betty11618/images/img/202409231558982.png)\n\n\n\n#### 有模型学习：纯规划\n\n![image-20240923152745296](https://raw.githubusercontent.com/betty11618/images/img/202409231558983.png)\n\n\n\n#### 有模型学习：Expert Iteration\n\n![image-20240923152847368](https://raw.githubusercontent.com/betty11618/images/img/202409231558984.png)\n\n\n\n#### 其他分类\n\n- 基于概率 - 基于价值\n- 回合更新 - 单步更新\n- 在线学习 - 离线学习\n\n\n\n# 实操步骤\n\n1. 收集数据 ---> 数据的数量和质量直接决定预测模型的好坏\n2. 数据准备 ---> 训练集、验证集、测试集\n3. 选择一个模型\n4. 训练\n5. 评估 ---> 指标：准确率、召回率、F值\n6. 参数调整\n7. 预测（开始使用）\n\n\n\n# 经典机器学习算法\n\n![image-20240923154529924](https://raw.githubusercontent.com/betty11618/images/img/202409231558985.png)\n\n\n\n# 心得\n\n得好好补补数学基础，争取在了解基本概念之后上实践，学完一轮再回来康康这个入门大纲吧。\n\n2024年9月23日"},{"title":"Hexo + github 搭建个人博客","url":"/2024/09/19/Hexo-github-搭建个人博客/","content":"\n\n\n## 安装node.js\n\n[Node.js — 在任何地方运行 JavaScript (nodejs.org)](https://nodejs.org/zh-cn)\n\n注意环境变量配置（安装时自动给配置了，不需要手动配）\n\n检查安装是否成功\n\n![image-20240815214704528](https://raw.githubusercontent.com/betty11618/images/img/202408152147586.png)\n\n\n\n## github创建仓库\n\n仓库命名 `xxx.github.io`  \n\n\n\n进入仓库，创建文件 `index.html`\n\n```html\n<h1>Hello Github Pages</h1>\n```\n\n\n\n# git for windows安装\n\n[Git for Windows](https://gitforwindows.org/)\n\n\n\n# github ssh配置\n\n使用 git 里面的 git-bash 输入\n\n```bash\nssh-keygen -t rsa -C \"注册github的邮箱\"\n```\n\n找到.ssh文件夹，打开id_rsa.pub文件\n\n将刚刚配置好的ssh，添加到自己的github账号中\n\n![image-20240906205238822](https://raw.githubusercontent.com/betty11618/images/img/202409062052872.png)\n\n\n\n# Hexo本地使用\n\n[Hexo](https://hexo.io/zh-cn/)\n\n创建一个新的文件夹，进入\n\n输入命令行下载hexo\n\n```\nnpm install hexo-cli -g\n```\n\n![image-20240906212001511](https://raw.githubusercontent.com/betty11618/images/img/202409062120544.png)\n\n验证证书失败，但通过复制数据的方式完成了初始化\n\n```\nnpm install\n```\n\n![image-20240906212233401](https://raw.githubusercontent.com/betty11618/images/img/202409062122432.png)\n\n```\nhexo s -p 5555\n```\n\n启动服务器，端口为5555\n\n![image-20240906212513009](https://raw.githubusercontent.com/betty11618/images/img/202409062125162.png)\n\n本地下载完成\n\n\n\n# sublime安装\n\n（其实芝士需要一个文本编辑器notepad++也行\n\n[Sublime Text - Text Editing, Done Right](https://www.sublimetext.com/)\n\n打开 Sublime Text 将 Hexo 下载所在的文件夹拖进去\n\n![image-20240908151243975](https://raw.githubusercontent.com/betty11618/images/img/202409192000094.png)\n\n保存\n\n![image-20240908151513279](https://raw.githubusercontent.com/betty11618/images/img/202409192000096.png)\n\n\n\n# 将Hexo发布到github上\n\n打开本地Hexo的配置文件\n\n![image-20240908180159477](https://raw.githubusercontent.com/betty11618/images/img/202409192000097.png)\n\n之前已经使用SSH进行github部署了，修改配置文件中 repo 的地址为使用SSH\n\n![image-20240908180649486](https://raw.githubusercontent.com/betty11618/images/img/202409192000098.png)\n\n此处填写自己的网址\n\n![image-20240908181024550](https://raw.githubusercontent.com/betty11618/images/img/202409192000099.png)\n\n生成，提交\n\n```\nhexo g\nhexo d\n```\n\n发现报错，查看Hexo文档后发现是缺少依赖\n\n```\nnpm install hexo-deployer-git --save\n```\n\n清除缓存，生成，提交\n\n```\nhexo clean\nhexo g\nhexo d\n```\n\n即可\n\n需要注意的是，github仓库是否启用了 github pages\n\n![image-20240908180347259](https://raw.githubusercontent.com/betty11618/images/img/202409192000100.png)\n\n最后我们尝试打开网站\n\n![image-20240918201756132](https://raw.githubusercontent.com/betty11618/images/img/202409182017345.png)\n\n成功发布\n\n\n\n# Hexo主题\n\n配置next主题\n\n[hexo-theme-next/docs/zh-CN/INSTALLATION.md at master · theme-next/hexo-theme-next (github.com)](https://github.com/theme-next/hexo-theme-next/blob/master/docs/zh-CN/INSTALLATION.md)\n\n跟着文档来就可以啦\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}]